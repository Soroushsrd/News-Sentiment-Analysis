{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# What is a LLM capable of ?\n",
    "Basically they are *\"... word generators\"*. So they can do any kind of question answering,\n",
    "summarization or entity extraction, (augmenting LLMs with connected APIs)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LLMS are distinguished by their *\"parameter size\"* (e.g. 10M parameters or 2B parameters)\n",
    "\n",
    "- Larger LLMs: More conceptual understanding --> best for complicated and multipurpose usage\n",
    "- Smaller LLms: Less conceptual but faster --> best for focused tasks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before the transformers (structures used in LLMs and ... Gen AI), we used RNNs.\n",
    "## What did we lack?\n",
    "- Memory: Longer sentences could not be analyzed\n",
    "- Multi-meaning words in sentences\n",
    "- Two/multiple sided sentences\n",
    "*We were able to solve these problems using Transformers*\n",
    "\n",
    "*Transformers* use self attention: a vector containing a selection of every word in a sentence (or even a whole text), called *Token Embedding Vector*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Max input size of a LLM is based on the size of its embedding vector*\n",
    "\n",
    "### Transformer Architecture Schematic\n",
    "\n",
    "![Transformer Architecture](https://miro.medium.com/v2/resize:fit:612/1*R5WJ_nO0gKy2yk3HHGorhg.jpeg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets break down some of its parts\n",
    "- Every input gets tokenized. each word being represented as a unique ID, based on a dictionary in use\n",
    "    the same tokenizer should be used for both training and generation\n",
    "- Token ID gets passed to a trainable embedding layer, so that each token ID --> high dimension vector --> represents the connection between words etc.\n",
    "- Add poositional encoding to preserve the word order\n",
    "- Feed the resulting vector (embed or positional) into a self attention layer; which learns to better capture the contextual relation between input words.\n",
    "    Self attention weight, learned during training helps the model to capture the relation (*and the importance of*, hence \"attention\") between each word with any\n",
    "    other word in the input\n",
    "    In most transformer architectures, the *multi-headed self attention* is used. Meaning that there are parallel self-attention layers,\n",
    "    which are independantly learning the importance off each encoding (embed+positional) in the sentences/text (12*100 number of multi-heads)\n",
    "    In fact, each attention head, learns an aspect of language (people, names, rhyme, etc)\n",
    "\n",
    "- after the attention weights have been applied to the data, it will be fed to a *feed forward* network and its output is a vector of logits with each\n",
    "    value proportional to its probability score, for each and every token for the tokenization  dictionary.\n",
    "- Now we can feed it to a softmax layer, outputing a vector with the length of the dictionary containing probability scores for each token.\n",
    "    The highest score in most cases is the next word that should be used to start or continue the word generation proccess"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
